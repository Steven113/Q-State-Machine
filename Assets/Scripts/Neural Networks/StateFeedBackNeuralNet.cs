//------------------------------------------------------------------------------
// <auto-generated>
//     This code was generated by a tool.
//     Runtime Version:4.0.30319.34209
//
//     Changes to this file may cause incorrect behavior and will be lost if
//     the code is regenerated.
// </auto-generated>
//------------------------------------------------------------------------------
using System;
using System.Collections.Generic;
using UnityEditor;
using UnityEngine;
using System.Runtime.Serialization;

namespace AssemblyCSharp
{
	[Serializable]
	public class StateFeedBackNeuralNet : EditorDisplay,IDeserializationCallback
	{
		int neuronID = 0;
		public List<List<Neuron>> neuronsByLayer = new List<List<Neuron>> (); //first list is inputs, last is outputs
		[NonSerialized]
		List<Vector2>
			scrollPosByLayer = new List<Vector2> ();
		int numStateCopiesToStore = 10;
		public List<Neuron> stateNeurons = new List<Neuron> ();
		public List<Neuron> actionNeurons = new List<Neuron> ();
		[NonSerialized]
		Vector2
			stateScrollPos = Vector2.zero;
		[NonSerialized]
		Vector2
			actionScrollPos = Vector2.zero;
		[NonSerialized]
		Vector2
			scrollPos;

		public StateFeedBackNeuralNet (int numInputs, int numHiddenLayers, int numOutputs)
		{
			neuronsByLayer.Add (new List<Neuron> (numInputs));
			for (int i = 0; i<numInputs; ++i) {
				neuronsByLayer [0].Add (new Neuron (++neuronID)); //add input neurons
			}
			scrollPosByLayer.Add (Vector2.zero);
			int numNeuronsForEachHiddenLayer = (int)((numInputs + numOutputs) * 0.5f);
			for (int i = 0; i<numHiddenLayers; ++i) {
				neuronsByLayer.Add (new List<Neuron> (numNeuronsForEachHiddenLayer));
				scrollPosByLayer.Add (Vector2.zero);
			}

			neuronsByLayer [neuronsByLayer.Count - 1].Add (new Neuron (++neuronID)); // add a single hidden layer neuron

			neuronsByLayer.Add (new List<Neuron> (numOutputs));
			scrollPosByLayer.Add (Vector2.zero);
			for (int i = 0; i<numOutputs; ++i) {
				neuronsByLayer [neuronsByLayer.Count - 1].Add (new Neuron (++neuronID)); //add output neurons
			}
		}

		public void GenerateConnectedNetwork ()
		{

			for (int i = 0; i<neuronsByLayer[0].Count; ++i) {
				for (int j = 0; j<neuronsByLayer[1].Count; ++j) {
					neuronsByLayer [0] [i].outputsIDs.Add (neuronsByLayer [1] [j].ID);
					neuronsByLayer [1] [j].inputsIDs.Add (neuronsByLayer [0] [i].ID);
					neuronsByLayer [1] [j].weights.Add (UnityEngine.Random.value);
				}
			}

			//neuronsByLayer [0].AddRange (stateNeurons);
			// connect action neurons to first hidden layer, adding weights to hidden layer for every action neuron
			for (int i = 0; i<actionNeurons.Count; ++i) {
				for (int j = 0; j<neuronsByLayer[1].Count; ++j) {
					actionNeurons [i].outputsIDs.Add (neuronsByLayer [1] [j].ID);
					neuronsByLayer [1] [j].inputsIDs.Add (actionNeurons [i].ID);
					neuronsByLayer [1] [j].weights.Add (UnityEngine.Random.value);
				}
			}
			neuronsByLayer [0].AddRange (actionNeurons);


			//neuronsByLayer [0].AddRange (actionNeurons);
			/*We now do a similar thing for the state neurons, except we duplicate the state neurons several times in order to have neurons to sample the past state 
			 * We also manually set the weights with a educated guess, to speed up learning. Since the state is sampled at a interval that 
			 * grows exponentially, the value of a state likely decreases exponentially. This is what we manually set the weights based on
			 */

			float weightToAdd = 0.5f;
			for (int k = 0; k<numStateCopiesToStore+1; ++k) {
				List<Neuron> copiedNeurons = new List<Neuron> (stateNeurons.Count);
				for (int n = 0; n<stateNeurons.Count; ++n) {
					Neuron tempNeuron = stateNeurons [n];
					copiedNeurons.Add (new Neuron (ref tempNeuron, ++neuronID));
				}
				for (int i = 0; i<copiedNeurons.Count; ++i) {
					for (int j = 0; j<neuronsByLayer[1].Count; ++j) {
						copiedNeurons [i].outputsIDs.Add (neuronsByLayer [1] [j].ID);
						neuronsByLayer [1] [j].inputsIDs.Add (copiedNeurons [i].ID);
						neuronsByLayer [1] [j].weights.Add (weightToAdd);
					}
				}
				weightToAdd /= 2.5f;
				neuronsByLayer [0].AddRange (copiedNeurons);
			}

			//connect all other layers
			for (int i = 2; i<neuronsByLayer.Count; ++i) {
				for (int j = 0; j<neuronsByLayer[i].Count; ++j) {
					for (int k = 0; k<neuronsByLayer[i-1].Count; ++k) {
						neuronsByLayer [i] [j].inputsIDs.Add (neuronsByLayer [i - 1] [k].ID);
						neuronsByLayer [i - 1] [k].outputsIDs.Add (neuronsByLayer [i] [j].ID);
						neuronsByLayer [i] [j].weights.Add (UnityEngine.Random.value);
					}
				}
			}
		}

		public int GetNumInputs {
			get {
				return neuronsByLayer [0].Count;
			}
		}

		//takes the given input and returns an array with the neural network output
		public float [] FeedInputs (float[] inputs)
		{
			Debug.Assert (inputs.Length == neuronsByLayer [0].Count);
			float [] output = new float[neuronsByLayer [neuronsByLayer.Count - 1].Count];
			for (int i = 0; i<inputs.Length; ++i) {
				neuronsByLayer [0] [i].transformedInput = inputs [i];
				neuronsByLayer [0] [i].summedInput = neuronsByLayer [0] [i].transformedInput;
			}

			for (int i = 1; i<neuronsByLayer.Count; ++i) {
				for (int j = 0; j<neuronsByLayer[i].Count; ++j) {
					for (int k = 0; k<neuronsByLayer[i][j].inputs.Count; ++k) {
						neuronsByLayer [i] [j].summedInput += neuronsByLayer [i] [j].inputs [k].transformedInput;
					}
					neuronsByLayer [i] [j].summedInput -= neuronsByLayer [i] [j].bias;
					switch (neuronsByLayer [i] [j].activationFunctionType) {
					case ActivationFunctionType.LINEAR:
						neuronsByLayer [i] [j].transformedInput = neuronsByLayer [i] [j].summedInput;
						break;
					case ActivationFunctionType.SIGMOID:
						neuronsByLayer [i] [j].transformedInput = 1f / (1f + Mathf.Exp (-neuronsByLayer [i] [j].summedInput));
						break;
					case ActivationFunctionType.TANH:
						float negExp = Mathf.Exp (-neuronsByLayer [i] [j].summedInput);
						float posExp = Mathf.Exp (neuronsByLayer [i] [j].summedInput);
						neuronsByLayer [i] [j].transformedInput = (posExp - negExp) / (posExp + negExp);
						break;
					}
				}
			}

			int numOutputs = output.Length;
			int lastLayerIndex = neuronsByLayer.Count - 1;
			for (int i = 0; i<numOutputs; ++i) {
				output [i] = neuronsByLayer [lastLayerIndex] [i].transformedInput;
			}

			return output;
		}

		public void BackPropagate (float[] expectedOutput, float learningRate)
		{
			int lastLayerIndex = neuronsByLayer.Count - 1;
			Debug.Assert (expectedOutput.Length == neuronsByLayer [lastLayerIndex].Count);

			//reset errors
			for (int i = lastLayerIndex; i>=0; ++i) {
				for (int j = 0; j<neuronsByLayer[i].Count; ++j) {
					neuronsByLayer [i] [j].error = 0;
				}
			}

			/*calculate errors. We must do this before updating the weights, so that the new weights don't mess with the error calculations
			 * Note that for efficiency reasons we calculate the error by taking every neuron in layer i and calculating it's contributions 
			 * to the neurons in layer i-1 that provide it input, since with the other way around you have to index which weight in the
			 * upper layer's weight list is the one that we must use to calculate the error
			 */
			for (int i = lastLayerIndex; i>=1; ++i) {

				for (int j = 0; j<neuronsByLayer[i].Count; ++j) {
					if (i == lastLayerIndex) {
						neuronsByLayer [i] [j].error = -(expectedOutput [j] - neuronsByLayer [i] [j].transformedInput); //for final layer error is just difference between expected and actual output
					} //else {
//						neuronsByLayer [i] [j].error = 0;
//						for (int k = 0; k<neuronsByLayer[i+1].Count; ++k) {
//							int weightIndex = -1;
//							if ((weightIndex = neuronsByLayer [i + 1] [k].outputs.IndexOf (neuronsByLayer [i] [j])) == -1) {
//								continue;
//							} else {
//								float deltaError = (neuronsByLayer [i + 1] [k].weights [weightIndex] * neuronsByLayer [i + 1] [k].error);
//								switch (neuronsByLayer [i + 1] [k].activationFunctionType) {
//									case ActivationFunctionType.LINEAR:
//										deltaError *= 1;
//										break;
//								case ActivationFunctionType.SIGMOID:
//									deltaError *= (neuronsByLayer [i + 1] [k].transformedInput*(1-neuronsByLayer [i + 1] [k].transformedInput));
//									break;
//								case ActivationFunctionType.TANH:
//									deltaError *= (1-neuronsByLayer [i + 1] [k].transformedInput*neuronsByLayer [i + 1] [k].transformedInput);
//									break;
//								}
//								neuronsByLayer [i] [j].error += deltaError;
//							}
//						}
//					}
					for (int k = 0; k<neuronsByLayer[i][j].inputs.Count; ++k){
						float deltaError = (neuronsByLayer [i] [j].weights [k] * neuronsByLayer [i] [j].error);
						switch (neuronsByLayer [i][j].activationFunctionType) {
						case ActivationFunctionType.LINEAR:
							deltaError *= 1;
							break;
						case ActivationFunctionType.SIGMOID:
							deltaError *= (neuronsByLayer [i] [j].transformedInput*(1-neuronsByLayer [i] [j].transformedInput));
							break;
						case ActivationFunctionType.TANH:
							deltaError *= (1-neuronsByLayer [i] [j].transformedInput*neuronsByLayer [i] [j].transformedInput);
							break;
						}
						neuronsByLayer[i][j].inputs[k].error+=deltaError;
					}


				}
				
			}

			//adjust weights
			for (int i = lastLayerIndex; i>=1; ++i) {
				float lc = neuronsByLayer[i].Count;
				for(int j = 0; j<lc; ++j){
					int ic = neuronsByLayer[i][j].inputs.Count;
					for (int k = 0; k<ic; ++k){
						float deltaWeight = -learningRate*neuronsByLayer[i][j].inputs[k].error * neuronsByLayer[i][j].transformedInput;
						switch (neuronsByLayer [i] [j].activationFunctionType) {
						case ActivationFunctionType.LINEAR:
							deltaWeight *= 1;
							break;
						case ActivationFunctionType.SIGMOID:
							deltaWeight *= (neuronsByLayer [i] [j].inputs[k].transformedInput*(1-neuronsByLayer [i] [j].inputs[k].transformedInput));
							break;
						case ActivationFunctionType.TANH:
							deltaWeight *= (1-neuronsByLayer [i] [j].inputs[k].transformedInput*neuronsByLayer [i] [j].inputs[k].transformedInput);
							break;
						}
						neuronsByLayer[i][j].weights[k]+=deltaWeight;
					}
				}
			}

		}

		public void ToEditorView ()
		{

			EditorGUILayout.LabelField ("How many copies of previous state to keep");
			numStateCopiesToStore = EditorGUILayout.IntField (numStateCopiesToStore);

			float spaceToCreate = 0;

			for (int i = 0; i<stateNeurons.Count; ++i) {
				spaceToCreate += 240;
				spaceToCreate += stateNeurons [i].outputsIDs.Count * 15f;
				spaceToCreate += stateNeurons [i].inputsIDs.Count * 15f;
			}

			for (int i = 0; i<actionNeurons.Count; ++i) {
				spaceToCreate += 240;
				spaceToCreate += actionNeurons [i].outputsIDs.Count * 15f;
				spaceToCreate += actionNeurons [i].inputsIDs.Count * 15f;
			}

			for (int i = 0; i<neuronsByLayer.Count; ++i) {
				for (int j = 0; j<neuronsByLayer[i].Count; ++j) {
					spaceToCreate += 240;
					spaceToCreate += neuronsByLayer [i] [j].outputsIDs.Count * 15f;
					spaceToCreate += neuronsByLayer [i] [j].inputsIDs.Count * 15f;
				}
			}

			scrollPos = EditorGUILayout.BeginScrollView (scrollPos, GUILayout.Width (1000), GUILayout.Height (spaceToCreate));

			ShowNeuronList (ref stateNeurons, ref stateScrollPos, "State Neurons", "Remove state neuron", "Add state neuron");
			ShowNeuronList (ref actionNeurons, ref actionScrollPos, "Action Neurons", "Remove action neuron", "Add action neuron");



			for (int i = 0; i<neuronsByLayer.Count; ++i) {
				List<Neuron> temp = neuronsByLayer [i];
				Vector2 tempS = scrollPosByLayer [i];
				if (i == 0) {
					ShowNeuronList (ref temp, ref tempS, "Input layer", "Remove input neuron", "Add input neuron");
				} else if (i == (neuronsByLayer.Count - 1)) {
					ShowNeuronList (ref temp, ref tempS, "Output layer", "Remove output neuron", "Add output neuron");
				} else {
					ShowNeuronList (ref temp, ref tempS, "Hidden layer " + (i), "Remove hidden neuron", "Add hidden neuron");
				}
				neuronsByLayer [i] = temp;
				scrollPosByLayer [i] = tempS;
			}

			if (GUILayout.Button ("Auto Link Neural Network")) {
				GenerateConnectedNetwork ();
			}

			EditorGUILayout.EndScrollView ();
		}

		public void ShowNeuronList (ref List<Neuron> list, ref Vector2 scrollPos, string label, string removeActionString, string addActionString)
		{
			EditorGUI.indentLevel += 3;
			//EditorGUILayout.EndVertical();
			EditorGUILayout.LabelField (label);

			//EditorGUILayout.BeginHorizontal();
			for (int j = 0; j<list.Count; ++j) {
				list [j].ToEditorView ();
				if (GUILayout.Button (removeActionString, GUILayout.MaxWidth (200))) {
					list.RemoveAt (j);
					//scrollPosByLayer.RemoveAt(j);
					--j;
				}
			}
			
			if (GUILayout.Button (addActionString, GUILayout.MaxWidth (200))) {
				list.Add (new Neuron (++neuronID));
				while (scrollPosByLayer.Count<neuronsByLayer.Count) {
					scrollPosByLayer.Add (Vector2.zero);
				}
			}
			//EditorGUILayout.EndHorizontal();

			
			//EditorGUILayout.BeginVertical();
			
			EditorGUI.indentLevel -= 3;
		}

//		public void OnBeforeSerialize(){
//
//		}

		void IDeserializationCallback.OnDeserialization (System.Object sender)
		{
			//Debug.Log ("Callback");
			stateScrollPos = Vector2.zero;
			actionScrollPos = Vector2.zero;
			scrollPos = Vector2.zero;
			scrollPosByLayer = new List<Vector2> (neuronsByLayer.Count);
			for (int i = 0; i<neuronsByLayer.Count; ++i) {
				scrollPosByLayer.Add (Vector2.zero);
			}
		}
	}
}

